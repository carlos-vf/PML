{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3df4c82a",
   "metadata": {},
   "source": [
    "## Importing dataset and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PRF\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "from utils import load_keel_dataset\n",
    "\n",
    "file_path_train=\"../keel_data/yeast-5-1tra.dat\"\n",
    "file_path_test=\"../keel_data/yeast-5-1tst.dat\"\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, label_map = load_keel_dataset(\n",
    "    train_path=file_path_train,\n",
    "    test_path=file_path_test,\n",
    "    already_split=True\n",
    ")\n",
    "print(\"Head of training data: \\n\", X_train[:5])\n",
    "print(\"Head of labels: \\n\", y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9657f6",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_train = [f'Feature_{i+1}' for i in range(X_train.shape[1])]\n",
    "\n",
    "df1 = pd.DataFrame(X_train, columns=feature_names_train)\n",
    "df1['Label'] = y_train\n",
    "df1['Dataset'] = 'X_train'\n",
    "\n",
    "feature_names_test = [f'Feature_{i+1}' for i in range(X_test.shape[1])]\n",
    "\n",
    "df2 = pd.DataFrame(X_test, columns=feature_names_test)\n",
    "df2['Label'] = y_test       # <-- Fix here: assign y_test to df2, not df1\n",
    "df2['Dataset'] = 'X_test'\n",
    "\n",
    "# Combine both train and test dataframes for plotting\n",
    "df_all = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Plot distributions of each feature side by side\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, feature in enumerate(feature_names_train):\n",
    "    plt.subplot(5, 4, i + 1)\n",
    "    sns.kdeplot(data=df_all, x=feature, hue='Dataset', common_norm=False, fill=True, alpha=0.4, bw_adjust=0.8)\n",
    "    plt.title(f\"Feature: {feature}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Plot label distribution (only for training set or combined if you want)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Label', data=df1, order=sorted(df1['Label'].unique()))\n",
    "plt.title(\"Label Distribution in Training Set\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Label', data=df2, order=sorted(df2['Label'].unique()))\n",
    "plt.title(\"Label Distribution in Test Set\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5537f119",
   "metadata": {},
   "source": [
    "#### Adding noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ae13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: THIS CODE MAKES LITTLE SENSE IF THERE ARE BOUNDS IN AN INTERVAL OF A FEATURE (LIKE IT HAS TO BE BETWEEN 0 AND 1)\n",
    "\n",
    "\n",
    "# X_train shape: (n_objects, n_features)\n",
    "n_objects, n_features = X_train.shape\n",
    "\n",
    "# Compute mean per feature across all objects\n",
    "feature_means = X_train.mean(axis=0)  # shape: (n_features,)\n",
    "\n",
    "# For each feature and object, sample noise mean from [-0.15 * mean, +0.15 * mean]\n",
    "noise_means = np.random.uniform(\n",
    "    low=-0.1 * feature_means,\n",
    "    high=0.1 * feature_means,\n",
    "    size=(n_objects, n_features)\n",
    ")\n",
    "\n",
    "# Choose variance (sigma^2)\n",
    "# 0.5 might be a bit arbitrary; you can set based on feature scale\n",
    "# A good heuristic: 10% of feature std deviation squared\n",
    "feature_stds = X_train.std(axis=0)\n",
    "noise_variances = (0.1 * feature_stds) ** 2  # shape: (n_features,)\n",
    "\n",
    "# We want variance per noise sample, so broadcast to shape (n_objects, n_features)\n",
    "noise_variances = np.tile(noise_variances, (n_objects, 1))\n",
    "\n",
    "# Sample Gaussian noise for each element with given mean and variance\n",
    "dX = np.random.normal(loc=noise_means, scale=np.sqrt(noise_variances))\n",
    "\n",
    "# Add noise to X_train\n",
    "X_train_noisy = X_train + dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df5b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Noise for labels (?), other types of noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507b7f59",
   "metadata": {},
   "source": [
    "#### Visualizing noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3ee88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [f'Feature_{i+1}' for i in range(X_train.shape[1])]\n",
    "\n",
    "df1 = pd.DataFrame(X_train, columns=feature_names)\n",
    "df1['Label'] = y_train\n",
    "df1['Dataset'] = 'X'\n",
    "\n",
    "df2 = pd.DataFrame(dX, columns=feature_names)\n",
    "df2['Dataset'] = 'dX'\n",
    "\n",
    "# Combine both for easier plotting\n",
    "df_all = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Plot distributions of each feature side by side\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, feature in enumerate(feature_names):\n",
    "    plt.subplot(5, 4, i + 1)\n",
    "    sns.kdeplot(data=df_all, x=feature, hue='Dataset', common_norm=False, fill=True, alpha=0.4)\n",
    "    plt.title(f\"Feature: {feature}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot label distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Label', data=df1, order=sorted(df1['Label'].unique()))\n",
    "plt.title(\"Label Distribution (Train set)\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb9786",
   "metadata": {},
   "source": [
    "## PDRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ec089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the PDRF algorithm\n",
    "\n",
    "accuracy_PDRF = 1.0 # Placeholder for the actual accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263b5940",
   "metadata": {},
   "source": [
    "## Other methods for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d222351",
   "metadata": {},
   "source": [
    "#### Probabilistic Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18702b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 10\n",
    "prf_cls = PRF.prf(n_estimators=n_trees,  bootstrap=True)\n",
    "prf_cls.fit(X=X_train, y=y_train, dX=dX)\n",
    "score = prf_cls.score(X_test, y=y_test)\n",
    "print('PRF Score: ', score)\n",
    "\n",
    "#Score is defined like this, so same as accuracy\n",
    "\n",
    "'''def score(self, X, y, dX=None):\n",
    "        y_pred = self.predict(X, dX)\n",
    "        score = (y_pred == (y)).sum()/len(y)\n",
    "        return score''' \n",
    "\n",
    "accuracy_PRF = score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21f575c",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a13eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=20, random_state=27)\n",
    "# Fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "accuracy_RF = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_RF:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eff3c7",
   "metadata": {},
   "source": [
    "#### Deep Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepforest import CascadeForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set random_state for reproducibility\n",
    "clf = CascadeForestClassifier(n_estimators=2, random_state=27)  # 2 estimators per layer by default\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_DF = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_DF:.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03001ea1",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c8e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "\n",
    "# Disable GPU (otherwise my PC explodes for some reason)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "np.random.seed(27)\n",
    "tf.random.set_seed(27)\n",
    "\n",
    "# Detect classification type\n",
    "unique_classes = np.unique(y_train)\n",
    "num_classes = len(unique_classes)\n",
    "is_binary = num_classes == 2\n",
    "\n",
    "# One-hot encode labels if multiclass\n",
    "if not is_binary:\n",
    "    y_train_cat = to_categorical(np.searchsorted(unique_classes, y_train))\n",
    "    y_test_cat = to_categorical(np.searchsorted(unique_classes, y_test))\n",
    "else:\n",
    "    y_train_cat = y_train\n",
    "    y_test_cat = y_test\n",
    "\n",
    "# Define the model builder function\n",
    "def create_model(hidden_units=32, dropout_rate=0.5, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    if is_binary:\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap with SciKeras\n",
    "model = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# Grid search parameters\n",
    "param_grid = {\n",
    "    'model__hidden_units': [32, 64],\n",
    "    'model__dropout_rate': [0.3, 0.5],\n",
    "    'model__optimizer': ['adam'],\n",
    "    'batch_size': [16],\n",
    "    'epochs': [20]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=1)\n",
    "grid_result = grid.fit(X_train, y_train_cat)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = grid_result.best_estimator_\n",
    "\n",
    "if is_binary:\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_test_true = y_test\n",
    "else:\n",
    "    y_pred_proba = best_model.predict_proba(X_test)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    y_test_true = np.searchsorted(unique_classes, y_test)\n",
    "\n",
    "# Output results\n",
    "accuracy_NN = accuracy_score(y_test_true, y_pred)\n",
    "print(\"Best params:\", grid_result.best_params_)\n",
    "print(f\"Accuracy: {accuracy_NN:.4f}\")\n",
    "print(classification_report(y_test_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97db7ae",
   "metadata": {},
   "source": [
    "#### Bayesian Logistic Regression (??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ff712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "183fef4f",
   "metadata": {},
   "source": [
    "#### Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5952d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Define the model\n",
    "svm_model = SVC()\n",
    "\n",
    "# 2. Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto']  \n",
    "}\n",
    "\n",
    "# 3. Grid search with cross-validation\n",
    "grid = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predict with best model\n",
    "best_svm = grid_result.best_estimator_\n",
    "y_pred = best_svm.predict(X_test)\n",
    "\n",
    "# 5. Report\n",
    "accuracy_KSVM = accuracy_score(y_test, y_pred)\n",
    "print(\"Best params:\", grid_result.best_params_)\n",
    "print(f\"Accuracy: {accuracy_KSVM:.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ffd28",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f6b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data\n",
    "data = {\n",
    "    'Model': ['PRF', 'PDRF', 'NN', 'KSVM', 'DF','RF'],\n",
    "    'Accuracy': [accuracy_PRF, accuracy_PDRF, accuracy_NN, accuracy_KSVM, accuracy_DF, accuracy_RF]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sort by Accuracy descending\n",
    "df = df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "# Define colors based on your request\n",
    "def get_color(model):\n",
    "    if model == 'PDRF':\n",
    "        return '#55bfc7'\n",
    "    elif model in ['PRF', 'DF']:\n",
    "        return 'lightgray'\n",
    "    else:\n",
    "        return 'lightgray'\n",
    "\n",
    "colors = df['Model'].apply(get_color)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.barplot(x='Model', y='Accuracy', data=df, palette=colors)\n",
    "\n",
    "# Remove top and right spines\n",
    "sns.despine()\n",
    "\n",
    "# Add space between title and plot\n",
    "plt.title('Model Accuracy Comparison', pad=20)\n",
    "\n",
    "# Set labels and limits\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Model')\n",
    "\n",
    "# Set tick font size\n",
    "ax.tick_params(axis='both', labelsize=11)\n",
    "\n",
    "# Add accuracy values on top of each bar, bold\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.annotate(f'{height:.2f}',\n",
    "                (p.get_x() + p.get_width() / 2., height),\n",
    "                ha='center', va='bottom',\n",
    "                fontsize=11, fontweight='bold', color='black',\n",
    "                xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668c736",
   "metadata": {},
   "source": [
    "#### Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a23d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Extract filename base: last part after `/`, before `.`, remove \"tra\" if present\n",
    "filename = Path(file_path_train).name  # 'my_dataset_tra.csv'\n",
    "basename = filename.rsplit('.', 1)[0].replace('tra', '')  # 'my_dataset_'\n",
    "\n",
    "# 2. Create output folder one level up\n",
    "output_dir = Path(file_path_train).parent.parent / 'accuracy_scores'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3. Save the dataframe\n",
    "output_path = output_dir / f'{basename}_noisy.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Accuracy data saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRF Kernel",
   "language": "python",
   "name": "pml-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
